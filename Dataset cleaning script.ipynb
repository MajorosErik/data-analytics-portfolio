{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7cc566c",
   "metadata": {},
   "source": [
    "# Ecommerce Events - Data Cleaning Pipeline\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "This notebook takes raw ecommerce event logs and turns them into a clean, consistent dataset that you can trust for analysis or dashboards.\n",
    "\n",
    "**Input files**\n",
    "\n",
    "- `2019-Oct.csv`\n",
    "- `2019-Nov.csv`\n",
    "\n",
    "Each row is an event with:\n",
    "\n",
    "- `event_time` - when the event happened\n",
    "- `event_type` - view / cart / remove_from_cart / purchase\n",
    "- `product_id` - product identifier\n",
    "- `category_id`, `category_code` - product category info\n",
    "- `brand` - product brand\n",
    "- `price` - price at the time of the event\n",
    "- `user_id` - user identifier\n",
    "- `user_session` - session identifier\n",
    "\n",
    "The raw files contain: missing IDs, bad prices, inconsistent text, extra event types, and scattered nulls. If you build anything on top of that, your numbers will drift and be hard to debug.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Build a simple, repeatable cleaning step that:\n",
    "\n",
    "- enforces basic data integrity  \n",
    "- removes obvious garbage  \n",
    "- standardizes key fields  \n",
    "- produces \"analysis-ready\" CSVs for BI or modeling\n",
    "\n",
    "## What the script enforces\n",
    "\n",
    "In the code below, the pipeline:\n",
    "\n",
    "- **Keeps only valid events**\n",
    "  - Drops rows without `user_id` or `product_id`\n",
    "  - Removes exact duplicate rows\n",
    "\n",
    "- **Standardizes core columns**\n",
    "  - Parses `event_time` to a proper datetime\n",
    "  - Lowercases and trims `brand` and `category_code`\n",
    "\n",
    "- **Applies clear business rules**\n",
    "  - Keeps only rows with `price > 0`\n",
    "  - Restricts `event_type` to the main funnel steps:\n",
    "    - `view`, `cart`, `remove_from_cart`, `purchase`\n",
    "\n",
    "- **Handles missing values consistently**\n",
    "  - Replaces remaining nulls with a single sentinel value (`\"-\"`)\n",
    "\n",
    "- **Scales to large files**\n",
    "  - Reads the CSVs in chunks, applies the same cleaning to each chunk, then writes cleaned outputs\n",
    "\n",
    "The next cell contains the full cleaning script. No manual tweaking is required: point it at the raw files, run it, and you get cleaned Oct and Nov event logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e89d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing 2019-Oct.csv...\n",
      "[DONE] Saved cleaned data to cleaned_2019-Oct.csv\n",
      "[ROWS] 42349875 rows written\n",
      "[VALIDATION PASSED] cleaned_2019-Oct.csv\n",
      "[TIME] 648.21 seconds\n",
      "\n",
      "[INFO] Processing 2019-Nov.csv...\n",
      "[DONE] Saved cleaned data to cleaned_2019-Nov.csv\n",
      "[ROWS] 67213478 rows written\n",
      "[VALIDATION PASSED] cleaned_2019-Nov.csv\n",
      "[TIME] 1114.77 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "#Load the necessary columns in chunks\n",
    "CHUNK_SIZE = 500_000\n",
    "COLUMNS = [\n",
    "    'event_time', 'event_type', 'product_id',\n",
    "    'category_id', 'category_code', 'brand',\n",
    "    'price', 'user_id', 'user_session'\n",
    "]\n",
    "\n",
    "#valid event types to keep\n",
    "VALID_EVENTS = ['view', 'cart', 'remove_from_cart', 'purchase']\n",
    "\n",
    "#clean a single chunk of data\n",
    "def clean_chunk(df):\n",
    "    #remove rows where user_id or product_id is missing\n",
    "    df = df.dropna(subset=['user_id', 'product_id'])\n",
    "\n",
    "    #remove duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    #convert event_time to proper datetime format\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'], errors='coerce')\n",
    "\n",
    "    #standardize brand and category_code (lowercase, no spaces)\n",
    "    df['brand'] = df['brand'].str.lower().str.strip()\n",
    "    df['category_code'] = df['category_code'].str.lower().str.strip()\n",
    "\n",
    "    #remove rows with price zero or negative\n",
    "    df = df[df['price'] > 0]\n",
    "\n",
    "    #keep the 4 event types\n",
    "    df = df[df['event_type'].isin(VALID_EVENTS)]\n",
    "\n",
    "    #replace any leftover nulls with a dash\n",
    "    df = df.fillna('-')\n",
    "\n",
    "    return df\n",
    "\n",
    "#check if cleaned data actually makes sense\n",
    "def validate_cleaned(df, filename):\n",
    "    issues = []\n",
    "\n",
    "    #check for any leftover nulls\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        issues.append(\"Null values remain\")\n",
    "\n",
    "    #check for invalid prices\n",
    "    if df['price'].le(0).any():\n",
    "        issues.append(\"Invalid prices found\")\n",
    "\n",
    "    #check if event_type has anything unexpected\n",
    "    if not df['event_type'].isin(VALID_EVENTS).all():\n",
    "        issues.append(\"Unexpected event types present\")\n",
    "\n",
    "    #check if the DataFrame is empty after cleaning\n",
    "    if df.shape[0] == 0:\n",
    "        issues.append(\"Resulting DataFrame is empty\")\n",
    "\n",
    "    #print issues if found, otherwise confirm it's clean\n",
    "    if issues:\n",
    "        print(f\"[VALIDATION FAILED] {filename}:\")\n",
    "        for issue in issues:\n",
    "            print(f\" - {issue}\")\n",
    "    else:\n",
    "        print(f\"[VALIDATION PASSED] {filename}\")\n",
    "\n",
    "#load, clean, and save one file\n",
    "def process_file(path_to_csv):\n",
    "    file_start = time.time()\n",
    "    name = Path(path_to_csv).stem\n",
    "    output_path = f\"cleaned_{name}.csv\"\n",
    "    processed_chunks = []\n",
    "\n",
    "    print(f\"[INFO] Processing {path_to_csv}...\")\n",
    "\n",
    "    #load file in chunks and clean each one\n",
    "    for chunk in pd.read_csv(path_to_csv, usecols=COLUMNS, chunksize=CHUNK_SIZE):\n",
    "        cleaned = clean_chunk(chunk)\n",
    "        processed_chunks.append(cleaned)\n",
    "\n",
    "    #combine all cleaned chunks\n",
    "    final_df = pd.concat(processed_chunks)\n",
    "\n",
    "    #save to cleaned CSV\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"[DONE] Saved cleaned data to {output_path}\")\n",
    "    print(f\"[ROWS] {len(final_df)} rows written\")\n",
    "\n",
    "    #run validation check on final output\n",
    "    validate_cleaned(final_df, output_path)\n",
    "\n",
    "    file_end = time.time()\n",
    "    print(f\"[TIME] {round(file_end - file_start, 2)} seconds\\n\")\n",
    "\n",
    "#process both csv files in one go\n",
    "def main():\n",
    "    files = [\"2019-Oct.csv\", \"2019-Nov.csv\"]\n",
    "    for file in files:\n",
    "        if os.path.exists(file):\n",
    "            process_file(file)\n",
    "        else:\n",
    "            print(f\"[ERROR] File not found: {file}\")\n",
    "\n",
    "#run script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2ad43",
   "metadata": {},
   "source": [
    "# What we get after cleaning\n",
    "\n",
    "After running the script, each raw file (for example `2019-Oct.csv`) becomes a cleaned file (`cleaned_2019-Oct.csv`) with the same columns but a much tighter definition of what a \"valid event\" is.\n",
    "\n",
    "## How the data changed\n",
    "\n",
    "The cleaned files now guarantee:\n",
    "\n",
    "- **Every event is attached to a real user and product**\n",
    "  - No rows with missing `user_id` or `product_id`\n",
    "\n",
    "- **No obvious double-counting**\n",
    "  - Exact duplicate rows are removed\n",
    "\n",
    "- **Consistent, usable fields**\n",
    "  - `event_time` is stored as a proper timestamp\n",
    "  - `brand` and `category_code` are normalized (lowercase, trimmed) so \"NIKE\", \"nike \" and \"Nike\" are the same thing\n",
    "\n",
    "- **Prices and events are sane**\n",
    "  - No non-positive prices\n",
    "  - `event_type` is limited to the main funnel actions:\n",
    "    - `view`, `cart`, `remove_from_cart`, `purchase`\n",
    "\n",
    "- **Missing data is explicit, not hidden**\n",
    "  - Remaining gaps are marked with `\"-\"` instead of silent nulls\n",
    "\n",
    "- **Basic validation is done for you**\n",
    "  - The script reports if:\n",
    "    - any nulls slipped through  \n",
    "    - any invalid prices remain  \n",
    "    - any unexpected event types exist  \n",
    "    - the cleaned file ended up empty\n",
    "\n",
    "## Why this matters\n",
    "\n",
    "With these constraints in place, you can safely:\n",
    "\n",
    "- Build funnels from view to cart to purchase by user, session, or product  \n",
    "- Analyze behavior by brand or category without fighting messy labels  \n",
    "- Feed the data into dashboards or models that assume \"one row = one valid event\"  \n",
    "- Re-run the same cleaning step on future months without touching the logic\n",
    "\n",
    "In short: raw logs go in, deterministic, analysis-ready event tables come out. This notebook documents both the rules and the code that enforces them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
